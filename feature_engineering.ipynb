{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOLeKo2Z2nAqH5cTsffr+5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiru883/Kaggle-IEEE-CIS-Fraud-Detection/blob/master/feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agAx8fROcqmy",
        "colab_type": "code",
        "outputId": "a43ad84c-48cd-4da9-d049-e4ee4528dfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, KFold\n",
        "from sklearn.feature_selection import RFECV"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk84hK2pjZC-",
        "colab_type": "text"
      },
      "source": [
        "# Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOtPbr0idRx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####LOAD DATASETS\n",
        "# train\n",
        "data_trainTR = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/train_transaction.csv\")\n",
        "data_trainID = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/train_identity.csv\")\n",
        "\n",
        "# test\n",
        "data_testTR = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/test_transaction.csv\")\n",
        "data_testID = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/test_identity.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOEIUo-PjTiI",
        "colab_type": "text"
      },
      "source": [
        "# Memory usage reduction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkaII_HMgHR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    mem_usage_before = np.around(df.memory_usage().sum() / 1028**2)\n",
        "    print(f\"Memory usage before: {mem_usage_before} MB\")\n",
        "\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'float':\n",
        "            mn, mx = df[column].min(), df[column].max()\n",
        "            if mn > -2147483648 or mx < 2147483648:\n",
        "                df[column] = df[column].astype('float32')\n",
        "\n",
        "        elif df[column].dtype == 'int':\n",
        "            mn, mx = df[column].min(), df[column].max()\n",
        "            if mn > -128 or mx < 127:\n",
        "                df[column] = df[column].astype('int8')\n",
        "            elif mn > -32000 or mx < 32000:\n",
        "                df[column] = df[column].astype('int16')\n",
        "            elif mn > -2147483648 or mx < 2147483648:\n",
        "                df[column] = df[column].astype('int32')\n",
        "\n",
        "        elif df[column].dtype == 'object':\n",
        "            df[column] = df[column].astype('category')\n",
        "\n",
        "    mem_usage_after = np.around(df.memory_usage().sum() / 1028**2)\n",
        "    print(f\"Memory usage after: {mem_usage_after} MB\")\n",
        "    print(f\"Optimization: {np.around(100*(1 - mem_usage_after/mem_usage_before))}%\")\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saMr3JXpuYuu",
        "colab_type": "text"
      },
      "source": [
        "# Main pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APSMQ9PMgP6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with all my hypotises\n",
        "def pipeline(data_TR, data_ID):\n",
        "    #useful functions\n",
        "    def decimal_places(x):\n",
        "        decimal_str = str(x)[str(x).find(\".\") +1:]\n",
        "        if decimal_str == \"0\":\n",
        "            return 0\n",
        "        decimal_len = len(str(int(decimal_str[::-1])))\n",
        "        return decimal_len if decimal_len < 5 else 5\n",
        "    \n",
        "    def email_map(email):\n",
        "        mapping= {'frontier.com':'frontier','frontiernet.net':'frontier','gmail':'gmail','gmail.com':'gmail','hotmail.co.uk':'hotmail','hotmail.com':'Microsoft','hotmail.de':'Microsoft',\n",
        "            'hotmail.es':'Microsoft','hotmail.fr':'Microsoft','icloud.com':'Apple','live.com':'Microsoft','live.com.mx':'Microsoft','live.fr':'Microsoft','mac.com':'Apple',\n",
        "            'netzero.com':'Netzero','netzero.net':'Netzero','outlook.com':'Microsoft','outlook.es':'Microsoft', 'yahoo.co.jp':'Yahoo','yahoo.co.uk':'Yahoo','yahoo.com':'Yahoo',\n",
        "            'yahoo.com.mx':'Yahoo','yahoo.de':'Yahoo','yahoo.es':'Yahoo','yahoo.fr':'Yahoo','ymail.com':'Yahoo', 'scranton.edu':'Scranton'}\n",
        "        if email in mapping.keys():\n",
        "            return mapping[email]\n",
        "        elif pd.isnull(email):\n",
        "            return 'NAN'\n",
        "        else:\n",
        "            return 'other'\n",
        "\n",
        "    def parse_id30(x):\n",
        "        devices_30_list = ['windows', 'ios', 'mac', 'android', 'linux']\n",
        "        if pd.isnull(x): \n",
        "            return 'NAN'\n",
        "        elif x.split()[0].lower() in devices_30_list: \n",
        "            return x.split()[0].lower()\n",
        "        else:\n",
        "            return 'NAN'\n",
        "        \n",
        "    def parse_id31(x):\n",
        "        devices_30_set = {'chrome', 'safari', 'ie', 'edge', 'firefox'}\n",
        "        if pd.isnull(x): \n",
        "            return 'NAN'\n",
        "        result = list(devices_30_set & set(x.split()))\n",
        "        if len(result) == 0:\n",
        "            return 'other'\n",
        "        else:\n",
        "            return result[0]\n",
        "        \n",
        "    def parse_id33(x):\n",
        "        devices_33_list = ['1334x750', '2436x1125', '1366x768', '1920x1080', '2208x1242']\n",
        "        if pd.isnull(x):\n",
        "            return 'NAN'\n",
        "        if x in devices_33_list:\n",
        "            return x\n",
        "        else:\n",
        "            return 'other'\n",
        "        \n",
        "    def parse_deviceinfo(x):\n",
        "        devices_info_list = ['windows', 'macos', 'ios', 'trident/7.0']\n",
        "        if pd.isnull(x):\n",
        "            return 'NAN'\n",
        "        x = x.split()[0].lower()\n",
        "        if x in devices_info_list:\n",
        "            return x\n",
        "        else:\n",
        "            return 'other'\n",
        "\n",
        "    df = pd.concat([data_TR.set_index('TransactionID'), data_ID.set_index('TransactionID')], axis=1).reset_index()\n",
        "    del data_TR, data_ID\n",
        "\n",
        "    #main pipeline\n",
        "    df['month'] = df['TransactionDT'] // (86400 * 30)\n",
        "    # transactionAmt features\n",
        "    df['TransactionAmt'] = df['TransactionAmt'].fillna(-999)\n",
        "    df['transaction_month'] = df.groupby(['month'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_meanq'] = pd.qcut(df['TransactionAmt'].median() - df['TransactionAmt'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    df['trans_std_negative'] = np.where((df['transaction_month'] < 0), 1, 0)\n",
        "    df['transaction_std'] = df['transaction_month'] / df.groupby(['month'])['TransactionAmt'].transform('std')\n",
        "    df['transaction_stdq'] = (df['TransactionAmt'].median() - df['TransactionAmt']) / df['TransactionAmt'].std()\n",
        "    df['transaction_stdq'] = pd.qcut(df['transaction_stdq'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    df['transaction_digits'] = df['TransactionAmt'].map(decimal_places)\n",
        "    df['transaction_count'] = df['TransactionAmt'].map(df['TransactionAmt'].value_counts())\n",
        "    df['transaction_count'] = pd.qcut(df['transaction_count'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    high_tr = df['TransactionAmt'].quantile([0.9]).to_list()[0]\n",
        "    lower_tr = df['TransactionAmt'].quantile([0.1]).to_list()[0]\n",
        "    df['outlier'] = np.where((df['TransactionAmt'] > high_tr) | (df['TransactionAmt'] < lower_tr), 1, 0)\n",
        "\n",
        "    #ProductCd\n",
        "    df['ProductCD'] = df['ProductCD'].fillna('NAN')\n",
        "    df['prod_stdq'] = pd.qcut(df.groupby([\"ProductCD\"])['TransactionAmt'].transform('median') - df['TransactionAmt'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['ProductCD'] = LabelEncoder().fit_transform(df['ProductCD'])\n",
        "    \n",
        "    #card1\n",
        "    df['card1'] = df['card1'].fillna(-999)\n",
        "    df['card1_count'] = df['card1'].map(df['card1'].value_counts())\n",
        "    df['card1_count_q'] = pd.qcut(df['card1_count'], [0.05, 0.2, 0.4, 0.6, 0.8, 0.95], labels=False).fillna(-1)\n",
        "    df['card1_frequency'] = df['card1'].map(df['card1'].value_counts() / df['card1'].shape[0])\n",
        "    df['card1_frequency'] = pd.qcut(df['card1_frequency'], [0.05, 0.2, 0.4, 0.6, 0.8, 0.95], labels=False).fillna(-1)\n",
        "    df['trans_card1_mean'] = df.groupby(['card1'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_card1_mean_rel'] = df.groupby(['card1'])['TransactionAmt'].transform('mean') / df['TransactionAmt']\n",
        "    df['card1_mean'] = df.groupby(['month'])['card1'].transform('mean') - df['card1']\n",
        "    df['card1_std'] = df.groupby(['month'])['card1'].transform('std') / df['card1_mean'] \n",
        "    df['card1_addr1_mean'] = df['card1'].astype('str') + '_' + df['addr1'].astype('str')\n",
        "    df['card1_addr1_mean'] = df.groupby(['card1_addr1_mean'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "\n",
        "    #card2\n",
        "    df['card2'] = df['card2'].fillna(-999)\n",
        "    df['card2_count'] = df['card2'].map(df['card2'].value_counts())\n",
        "    df['card2_out'] = pd.qcut(df['card2_count'], [0.05, 0.15, 0.85, 0.95], labels=False).fillna(-1)\n",
        "    df['card2_q'] = pd.qcut(df['card2_count'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "\n",
        "    #card3\n",
        "    df['card3'] = df['card3'].fillna(-999)\n",
        "    df['card3_o'] = np.where((df['card3'] < df['card3'].quantile(0.1)) | (df['card3'] > df['card3'].quantile(0.9)), 1, 0)\n",
        "    \n",
        "    #card4\n",
        "    df['card4'] = df['card4'].fillna('NAN')\n",
        "    df['card4_count'] = df['card4'].map(df['card4'].value_counts())\n",
        "    df['trans_card_mean'] = df.groupby(['card4'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_card_mean_q'] = pd.qcut(df['trans_card_mean'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['trans_card_std'] = df.groupby(['card4'])['TransactionAmt'].transform('std') / df['trans_card_mean']\n",
        "    df['trans_card_std_q'] = pd.qcut(df['trans_card_std'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['card4_trans_mean'] = df['TransactionAmt'].astype('str') + '_' + df['card4'].astype('str')\n",
        "    df['card4_trans_mean'] = df.groupby(['card4_trans_mean'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['card4'] = LabelEncoder().fit_transform(df['card4'])\n",
        "\n",
        "    #card5\n",
        "    df['card5'] = df['card5'].fillna(-999)\n",
        "    df['card5_o'] = np.where(((df['card5'] < df['card5'].quantile(0.05)) | (df['card5'] > df['card5'].quantile(0.95))), 1, 0)\n",
        "\n",
        "    #card6\n",
        "    df['card6'] = df['card6'].fillna('NAN')\n",
        "    df['trans_card_med'] = df.groupby(['card6'])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "    df['card6'] = LabelEncoder().fit_transform(df['card6'])\n",
        "\n",
        "    #addr1\n",
        "    df['addr1'] = df['addr1'].fillna(-999)\n",
        "    df['addr1_isnull'] = df['addr1'].isnull().astype(int)\n",
        "    df['addr1_frequency'] = df['addr1'].map(df['addr1'].value_counts())\n",
        "    df['addr_trans'] = df.groupby(['addr1'])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "    df['addr_trans_q'] = pd.qcut(df['addr_trans'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['addr_card_med'] = df.groupby(['addr1'])['card1'].transform('median') - df['card1']\n",
        "    df['addr_card'] = df.groupby(['addr1'])['card1'].transform('median') - df['card1']\n",
        "    df['addr_card_q'] = pd.qcut(df['addr_card'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['addrcard'] = df['addr1'].astype(str) + \"_\" + df['card1'].astype(str)\n",
        "    df['addrcard'] = LabelEncoder().fit_transform(df['addrcard'])\n",
        "\n",
        "    #addr2 & dist1 & dist2\n",
        "    df['addr2'] = df['addr2'].fillna(-999)\n",
        "    df['dist1'] = df['dist1'].fillna(-999)\n",
        "    df['dist2'] = df['dist2'].fillna(-999)\n",
        "\n",
        "    #p_emaildomain\n",
        "    df['P_emaildomain'] = LabelEncoder().fit_transform(df['P_emaildomain'].map(email_map))\n",
        "\n",
        "    #r_emaildomain\n",
        "    df['R_emaildomain'] = LabelEncoder().fit_transform(df['R_emaildomain'].map(email_map))\n",
        "\n",
        "    #Cs\n",
        "    for Ci in range(1, 15):\n",
        "        c = \"C\" + str(Ci)\n",
        "        df[c] = df[c].fillna(-999)\n",
        "        df[c + '_trans_div'] = df.groupby([c])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "        df[c + '_trans_div_q'] = pd.qcut(df[c + '_trans_div'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "\n",
        "    #Ds\n",
        "    for Di in range(1, 16):\n",
        "        d = \"D\" + str(Di)\n",
        "        df[d] = df[d].fillna(-999)\n",
        "    \n",
        "    #Ms\n",
        "    for Mi in range(1, 10):\n",
        "        m = \"M\" + str(Mi)\n",
        "        df[m] = df[m].fillna(\"NAN\")\n",
        "        df[m] = LabelEncoder().fit_transform(df[m])\n",
        "\n",
        "    #Vs\n",
        "    for Vi in range(1, 340):\n",
        "        v = \"V\" + str(Vi)\n",
        "        df[v] = df[v].fillna(-999)\n",
        "\n",
        "    #ids\n",
        "    for Ii in range(1, 39):\n",
        "        i = \"id_0\" if Ii < 10 else \"id_\"\n",
        "        i += str(Ii)\n",
        "        if df[i].dtype == 'object':\n",
        "            if i == \"id_30\":\n",
        "                df[i] = df[i].map(parse_id30)\n",
        "            elif i == \"id_31\":\n",
        "                df[i] = df[i].map(parse_id31)\n",
        "            elif i == 'id_33':\n",
        "                df[i] = df[i].map(parse_id33)\n",
        "            df[i] = df[i].fillna('NAN')\n",
        "            df[i] = LabelEncoder().fit_transform(df[i])\n",
        "        else:\n",
        "            df[i] = df[i].fillna(-999)\n",
        "\n",
        "    #test functions\n",
        "    features = ['TransactionAmt', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'D15', 'C13', 'addr1', 'dist1', 'R_emaildomain', 'M2', 'M4']\n",
        "    features_combinations = list(itertools.combinations(features, 2))\n",
        "    for f1, f2 in features_combinations:\n",
        "        feature_name = f1 + '_' + f2\n",
        "        df[feature_name] = df[f1].astype('str') + '_' + df[f2].astype('str')\n",
        "        df[feature_name] = df[feature_name].map(df[feature_name].value_counts())\n",
        "\n",
        "    #deviceType & deviceInfo\n",
        "    df['DeviceType'] = LabelEncoder().fit_transform(df['DeviceType'].fillna(\"NAN\"))\n",
        "    df['DeviceInfo'] = LabelEncoder().fit_transform(df['DeviceInfo'].map(parse_deviceinfo))\n",
        "\n",
        "    df.sort_values(by=[\"TransactionDT\"], ascending=True).reset_index()\n",
        "    df = df.drop([\"TransactionDT\", \"TransactionID\", \"month\"], axis=1)\n",
        "\n",
        "    gc.collect()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdH20PO2Mixa",
        "colab_type": "text"
      },
      "source": [
        "# RFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roOxLlZuaBOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e47a482e-4eb8-4b9d-a797-90489b92e276"
      },
      "source": [
        "test = pipeline(data_trainTR.drop(['isFraud'], axis=1), data_trainID)\n",
        "test = reduce_mem_usage(test)\n",
        "X = test.to_numpy()\n",
        "y = data_trainTR['isFraud'].to_numpy().reshape(-1, 1)\n",
        "\n",
        "#RFE estimator\n",
        "clf = lgb.LGBMClassifier(\n",
        "    num_leaves = 490,\n",
        "    n_estimators = 300,\n",
        "    metrics = 'auc',\n",
        "    objective = 'binary'\n",
        ")\n",
        "\n",
        "#RFECV\n",
        "rfecv = RFECV(\n",
        "    estimator=clf,\n",
        "    step=10,\n",
        "    cv=TimeSeriesSplit(),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "rfecv.fit(X, y)\n",
        "\n",
        "features = [x for x in test.columns[rfecv.ranking_ == 1]]\n",
        "print(f\"Number of features: {len(features)}\")\n",
        "print(\"\\nFeatures:\\n\", features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage before: 2624.0 MB\n",
            "Memory usage after: 1087.0 MB\n",
            "Optimization: 59.0%\n",
            "Fitting estimator with 587 features.\n",
            "Fitting estimator with 577 features.\n",
            "Fitting estimator with 567 features.\n",
            "Fitting estimator with 557 features.\n",
            "Fitting estimator with 547 features.\n",
            "Fitting estimator with 537 features.\n",
            "Fitting estimator with 527 features.\n",
            "Fitting estimator with 517 features.\n",
            "Fitting estimator with 507 features.\n",
            "Fitting estimator with 497 features.\n",
            "Fitting estimator with 487 features.\n",
            "Fitting estimator with 477 features.\n",
            "Fitting estimator with 467 features.\n",
            "Fitting estimator with 457 features.\n",
            "Fitting estimator with 447 features.\n",
            "Fitting estimator with 437 features.\n",
            "Fitting estimator with 427 features.\n",
            "Fitting estimator with 417 features.\n",
            "Fitting estimator with 407 features.\n",
            "Fitting estimator with 397 features.\n",
            "Fitting estimator with 387 features.\n",
            "Fitting estimator with 377 features.\n",
            "Fitting estimator with 367 features.\n",
            "Fitting estimator with 357 features.\n",
            "Fitting estimator with 347 features.\n",
            "Fitting estimator with 337 features.\n",
            "Fitting estimator with 327 features.\n",
            "Fitting estimator with 317 features.\n",
            "Fitting estimator with 307 features.\n",
            "Fitting estimator with 297 features.\n",
            "Fitting estimator with 287 features.\n",
            "Fitting estimator with 277 features.\n",
            "Fitting estimator with 267 features.\n",
            "Fitting estimator with 257 features.\n",
            "Fitting estimator with 247 features.\n",
            "Fitting estimator with 237 features.\n",
            "Fitting estimator with 227 features.\n",
            "Fitting estimator with 217 features.\n",
            "Fitting estimator with 207 features.\n",
            "Fitting estimator with 197 features.\n",
            "Fitting estimator with 187 features.\n",
            "Fitting estimator with 177 features.\n",
            "Fitting estimator with 167 features.\n",
            "Fitting estimator with 157 features.\n",
            "Fitting estimator with 147 features.\n",
            "Fitting estimator with 137 features.\n",
            "Fitting estimator with 127 features.\n",
            "Fitting estimator with 117 features.\n",
            "Fitting estimator with 107 features.\n",
            "Fitting estimator with 97 features.\n",
            "Fitting estimator with 87 features.\n",
            "Fitting estimator with 77 features.\n",
            "Fitting estimator with 67 features.\n",
            "Fitting estimator with 57 features.\n",
            "Fitting estimator with 47 features.\n",
            "Fitting estimator with 37 features.\n",
            "Fitting estimator with 27 features.\n",
            "Fitting estimator with 17 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 587 features.\n",
            "Fitting estimator with 577 features.\n",
            "Fitting estimator with 567 features.\n",
            "Fitting estimator with 557 features.\n",
            "Fitting estimator with 547 features.\n",
            "Fitting estimator with 537 features.\n",
            "Fitting estimator with 527 features.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqDP-gj6rA6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}